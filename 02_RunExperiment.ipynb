{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment code for standard DTW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this notebook only contains code for DTW1-5. Other variations of DTW are implemented in another folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lb\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import time\n",
    "import gc\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DTWVariations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "cimport cython\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "DTYPE_INT32 = np.int32\n",
    "ctypedef np.int32_t DTYPE_INT32_t\n",
    "\n",
    "DTYPE_FLOAT = np.float64\n",
    "ctypedef np.float64_t DTYPE_FLOAT_t\n",
    "\n",
    "cdef DTYPE_FLOAT_t MAX_FLOAT = float('inf')\n",
    "\n",
    "# careful, without bounds checking can mess up memory - also can't use negative indices I think (like x[-1])\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "def DTW_Cost_To_AccumCostAndSteps(Cin, parameter, WarpMax=None):\n",
    "    '''\n",
    "    Inputs\n",
    "        C: The cost Matrix\n",
    "    '''\n",
    "\n",
    "\n",
    "    '''\n",
    "    Section for checking and catching errors in the inputs\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] C\n",
    "    try:\n",
    "        C = np.array(Cin, dtype=DTYPE_FLOAT)\n",
    "    except TypeError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the cost matrix is wrong - please pass in a 2-d numpy array\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "    except ValueError:\n",
    "        print(bcolors.FAIL + \"FAILURE: The type of the elements in the cost matrix is wrong - please have each element be a float (perhaps you passed in a matrix of ints?)\" + bcolors.ENDC)\n",
    "        return [-1, -1, -1]\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dn\n",
    "    cdef np.ndarray[np.uint32_t, ndim=1] dm\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=1] dw\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    # dn loading and exception handling\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        try:\n",
    "\n",
    "            dn = np.array(parameter['dn'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dn (row steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"The type of the elements in dn (row steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=np.uint32)\n",
    "    # dm loading and exception handling\n",
    "    if 'dm'  in parameter.keys():\n",
    "        try:\n",
    "            dm = np.array(parameter['dm'], dtype=np.uint32)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dm (col steps) is wrong - please pass in a 1-d numpy array that holds uint32s\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of the elements in dm (col steps) is wrong - please have each element be a uint32 (perhaps you passed a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.uint32)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        print(bcolors.FAIL + \"dm (col steps) was not passed in (gave default value [1,0,1]) \" + bcolors.ENDC)\n",
    "        dm = np.array([1, 0, 1], dtype=np.uint32)\n",
    "    # dw loading and exception handling\n",
    "    if 'dw'  in parameter.keys():\n",
    "        try:\n",
    "            dw = np.array(parameter['dw'], dtype=DTYPE_FLOAT)\n",
    "        except TypeError:\n",
    "            print(bcolors.FAIL + \"FAILURE: The type of dw (step weights) is wrong - please pass in a 1-d numpy array that holds floats\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "        except ValueError:\n",
    "            print(bcolors.FAIL + \"FAILURE:The type of the elements in dw (step weights) is wrong - please have each element be a float (perhaps you passed ints or a long?). You can specify this when making a numpy array like: np.array([1,2,3],dtype=np.float64)\" + bcolors.ENDC)\n",
    "            return [-1, -1, -1]\n",
    "    else:\n",
    "        dw = np.array([1, 1, 1], dtype=DTYPE_FLOAT)\n",
    "        print(bcolors.FAIL + \"dw (step weights) was not passed in (gave default value [1,1,1]) \" + bcolors.ENDC)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Section where types are given to the variables we're going to use \n",
    "    '''\n",
    "    # create matrices to store our results (D and E)\n",
    "    cdef DTYPE_INT32_t numRows = C.shape[0] # only works with np arrays, use np.shape(x) will work on lists? want to force to use np though?\n",
    "    cdef DTYPE_INT32_t numCols = C.shape[1]\n",
    "    cdef DTYPE_INT32_t numDifSteps = np.size(dw)\n",
    "\n",
    "    cdef unsigned int maxRowStep = max(dn)\n",
    "    cdef unsigned int maxColStep = max(dm)\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] steps = np.zeros((numRows,numCols), dtype=np.uint32)\n",
    "    cdef np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost = np.ones((maxRowStep + numRows, maxColStep + numCols), dtype=DTYPE_FLOAT) * MAX_FLOAT\n",
    "\n",
    "    cdef DTYPE_FLOAT_t bestCost\n",
    "    cdef DTYPE_INT32_t bestCostIndex\n",
    "    cdef DTYPE_FLOAT_t costForStep\n",
    "    cdef unsigned int row, col\n",
    "    cdef unsigned int stepIndex\n",
    "\n",
    "    '''\n",
    "    The start of the actual algorithm, now that all our variables are set up\n",
    "    '''\n",
    "    # initializing the cost matrix - depends on whether its subsequence DTW\n",
    "    # essentially allow us to hop on the bottom anywhere (so could start partway through one of the signals)\n",
    "    if parameter['SubSequence']:\n",
    "        for col in range(numCols):\n",
    "            accumCost[maxRowStep, col + maxColStep] = C[0, col]\n",
    "    else:\n",
    "        accumCost[maxRowStep, maxColStep] = C[0,0]\n",
    "\n",
    "    # ADDED\n",
    "    remove_x = False\n",
    "    remove_y = False\n",
    "\n",
    "    # filling the accumulated cost matrix\n",
    "    for row in range(maxRowStep, numRows + maxRowStep, 1):\n",
    "        if WarpMax and row % WarpMax == 0 and row > 1: # ADDED\n",
    "            remove_x = True\n",
    "\n",
    "        for col in range(maxColStep, numCols + maxColStep, 1):\n",
    "            if WarpMax and col % WarpMax == 0: # ADDED\n",
    "                remove_y = True\n",
    "            \n",
    "            bestCost = accumCost[<unsigned int>row, <unsigned int>col] # initialize with what's there - so if is an entry point, then can start low\n",
    "            bestCostIndex = 0\n",
    "            # go through each step, find the best one\n",
    "            for stepIndex in range(numDifSteps):\n",
    "                if remove_x and dn[(stepIndex)]==1 and dm[(stepIndex)]==0: # ADDED\n",
    "                    continue\n",
    "                if remove_y and dm[(stepIndex)]==1 and dn[(stepIndex)]==0:\n",
    "                    continue\n",
    "                costForStep = accumCost[<unsigned int>((row - dn[(stepIndex)])), <unsigned int>((col - dm[(stepIndex)]))] + dw[stepIndex] * C[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)]\n",
    "                if costForStep < bestCost:\n",
    "                    bestCost = costForStep\n",
    "                    bestCostIndex = stepIndex\n",
    "            \n",
    "            # save the best cost and best cost index\n",
    "            accumCost[row, col] = bestCost\n",
    "            steps[<unsigned int>(row - maxRowStep), <unsigned int>(col - maxColStep)] = bestCostIndex\n",
    "            remove_y = False\n",
    "        remove_x = False\n",
    "\n",
    "    # return the accumulated cost along with the matrix of steps taken to achieve that cost\n",
    "    return [accumCost[maxRowStep:, maxColStep:], steps]\n",
    "\n",
    "@cython.boundscheck(False) # turn off bounds-checking for entire function\n",
    "\n",
    "def DTW_GetPath(np.ndarray[DTYPE_FLOAT_t, ndim=2] accumCost, np.ndarray[np.uint32_t, ndim=2] stepsForCost, parameter):\n",
    "    '''\n",
    "\n",
    "    Parameter should have: 'dn', 'dm', 'dw', 'SubSequence'\n",
    "    '''\n",
    "\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dn\n",
    "    cdef np.ndarray[unsigned int, ndim=1] dm\n",
    "    cdef np.uint8_t subseq\n",
    "    cdef np.int32_t startCol # added\n",
    "    # make sure dn, dm, and dw are setup\n",
    "    if ('dn'  in parameter.keys()):\n",
    "        dn = parameter['dn']\n",
    "    else:\n",
    "        dn = np.array([1, 1, 0], dtype=DTYPE_INT32)\n",
    "    if 'dm'  in parameter.keys():\n",
    "        dm = parameter['dm']\n",
    "    else:\n",
    "        dm = np.array([1, 0, 1], dtype=DTYPE_INT32)\n",
    "    if 'SubSequence' in parameter.keys():\n",
    "        subseq = parameter['SubSequence']\n",
    "    else:\n",
    "        subseq = 0\n",
    "    \n",
    "    # added START\n",
    "    if 'startCol' in parameter.keys(): \n",
    "        startCol = parameter['startCol']\n",
    "    else:\n",
    "        startCol = -1\n",
    "    # added END\n",
    "\n",
    "    cdef np.uint32_t numRows\n",
    "    cdef np.uint32_t numCols\n",
    "    cdef np.uint32_t curRow\n",
    "    cdef np.uint32_t curCol\n",
    "    cdef np.uint32_t endCol\n",
    "    cdef DTYPE_FLOAT_t endCost\n",
    "\n",
    "    numRows = accumCost.shape[0]\n",
    "    numCols = accumCost.shape[1]\n",
    "\n",
    "    # either start at the far corner (non sub-sequence)\n",
    "    # or start at the lowest cost entry in the last row (sub-sequence)\n",
    "    # where all of the signal along the row has been used, but only a \n",
    "    # sub-sequence of the signal along the columns has to be used\n",
    "    curRow = numRows - 1\n",
    "    if subseq:\n",
    "        curCol = np.argmin(accumCost[numRows - 1, :])\n",
    "    else:\n",
    "        curCol = numCols - 1\n",
    "        \n",
    "    # added - if specified, overrides above\n",
    "    if startCol >= 0:\n",
    "        curCol = startCol\n",
    "\n",
    "    endCol = curCol\n",
    "    endCost = accumCost[curRow, curCol]\n",
    "\n",
    "    cdef np.uint32_t curRowStep\n",
    "    cdef np.uint32_t curColStep\n",
    "    cdef np.uint32_t curStepIndex\n",
    "\n",
    "\n",
    "    cdef np.ndarray[np.uint32_t, ndim=2] path = np.zeros((2, numRows + numCols), dtype=np.uint32) # make as large as could need, then chop at the end\n",
    "    path[0, 0] = curRow\n",
    "    path[1, 0] = curCol\n",
    "\n",
    "    cdef np.uint32_t stepsInPath = 1 # starts at one, we add in one before looping\n",
    "    cdef np.uint32_t stepIndex = 0\n",
    "    cdef np.int8_t done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "    while not done:\n",
    "        if accumCost[curRow, curCol] == MAX_FLOAT:\n",
    "            print('A path is not possible')\n",
    "            break\n",
    "\n",
    "        # you're done if you've made it to the bottom left (non sub-sequence)\n",
    "        # or just the bottom (sub-sequence)\n",
    "        # find the step size\n",
    "        curStepIndex = stepsForCost[curRow, curCol]\n",
    "        curRowStep = dn[curStepIndex]\n",
    "        curColStep = dm[curStepIndex]\n",
    "        # backtrack by 1 step\n",
    "        curRow = curRow - curRowStep\n",
    "        curCol = curCol - curColStep\n",
    "        # add your new location onto the path\n",
    "        path[0, stepsInPath] = curRow\n",
    "        path[1, stepsInPath] = curCol\n",
    "        stepsInPath = stepsInPath + 1\n",
    "        # check to see if you're done\n",
    "        done = (subseq and curRow == 0) or (curRow == 0 and curCol == 0)\n",
    "\n",
    "    # reverse the path (a matrix with two rows) and return it\n",
    "    return [np.fliplr(path[:, 0:stepsInPath]), endCol, endCost]\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW(featfile1, featfile2, steps, weights, downsample, outfile = None, profile = False):\n",
    "    \n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "    # if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "    #     if outfile:\n",
    "    #         pickle.dump(None, open(outfile, 'wb'))\n",
    "    #     return None\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "    times.append(time.time())\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW_batch(querylist, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, alignment, warpMax=None): # alignment is e.g. 'DTW1'\n",
    "    \n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # prep inputs for parallelization\n",
    "    inputs = []\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(' ')\n",
    "            assert len(parts) == 2\n",
    "            featfile1 = (featdir1 / parts[0]).with_suffix('.npy')\n",
    "            featfile2 = (featdir2 / parts[1]).with_suffix('.npy')\n",
    "            queryid = os.path.basename(parts[0]) + '__' + os.path.basename(parts[1])\n",
    "            outfile = (outdir / queryid).with_suffix('.pkl')\n",
    "            if os.path.exists(outfile):\n",
    "                print(f\"Skipping {outfile}\")\n",
    "            else:\n",
    "                if warpMax:\n",
    "                    inputs.append((featfile1, featfile2, steps, weights, downsample, warpMax, outfile)) # for selective transitions, need an extra argument warpMax\n",
    "                else:\n",
    "                    inputs.append((featfile1, featfile2, steps, weights, downsample, outfile))\n",
    "\n",
    "    # process files in parallel\n",
    "    pool = multiprocessing.Pool(processes = n_cores)\n",
    "    pool.starmap(alignment, inputs)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for other variations of DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW_sub(F1, F2, downsample, steps, weights, outfile, profile):\n",
    "    '''identical to alignDTW, but does not load in the files and instead takes in matrices'''\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "    times.append(time.time())\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective Transition alignDTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignDTW_selective(featfile1, featfile2, steps, weights, downsample, warpMax, outfile = None, profile = False):\n",
    "    # set default warpMax to 3\n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "    # if max(F1.shape[1], F2.shape[1]) / min(F1.shape[1], F2.shape[1]) >= 2: # no valid path possible\n",
    "    #     if outfile:\n",
    "    #         pickle.dump(None, open(outfile, 'wb'))\n",
    "    #     return None\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters, warpMax)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "    times.append(time.time())\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_adaptiveWeight1(featfile1, featfile2, steps, weights=None, downsample=None, outfile = None, profile = False):\n",
    "    '''DTW but with a weighting scheme ensures that both axes contribute the same weighted Manhattan distance cost'''\n",
    "    # calculate weights:\n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "\n",
    "    w1, w2 = None, None # keep track of which is the longer sequence\n",
    "\n",
    "    # calculate weight ratios\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        Lmax, Lmin = F1.shape[1], F2.shape[1]\n",
    "        w1, w2 = Lmax/Lmin, 1 # calulate ratios\n",
    "    else:\n",
    "        Lmax, Lmin = F2.shape[1], F1.shape[1]\n",
    "        w1, w2 = 1, Lmax/Lmin # calulate ratios\n",
    "\n",
    "    # calculate weights for each step\n",
    "    weights = []\n",
    "    # for step in steps, calculate weight\n",
    "    for step in steps: # step is e.g. [1,1]\n",
    "        wy = step[0]*w2 # step[0] is in Lmin direction\n",
    "        wx = step[1]*w1\n",
    "        weights.append(wy + wx)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    return alignDTW_sub(F1, F2, downsample, steps, weights, outfile, profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW_adaptiveWeight2(featfile1, featfile2, steps, weights=None, downsample=None, outfile = None, profile = False):\n",
    "    '''DTW but with a different weighting scheme that accounts for the length of the sequences'''\n",
    "    # calculate weights w1, w2, w3\n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "\n",
    "    w1, w2, w3 = None, None, None # keep track of which is the longer sequence\n",
    "\n",
    "    # calculate weight ratios\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        Lmax, Lmin = F1.shape[1], F2.shape[1]\n",
    "        w1, w2 = Lmax/Lmin, 1 # calulate ratios\n",
    "    else:\n",
    "        Lmax, Lmin = F2.shape[1], F1.shape[1]\n",
    "        w1, w2 = 1, Lmax/Lmin # calulate ratios\n",
    "    w3 = 1 + Lmax/Lmin\n",
    "    weights = np.array([w1, w2, w3])\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    return alignDTW_sub(F1, F2, downsample, steps, weights, outfile, profile)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling, downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW1_downsampleQuantized(featfile1, featfile2, steps, weights, downsample, outfile = None, profile = False):\n",
    "    '''Downsample longer sequence. For each position in the downsampled sequence, \n",
    "    assign the nearest feature vector (to handle fractional positions).'''\n",
    "    \n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "\n",
    "    shorter, longer = None, None\n",
    "    \n",
    "    # boolean to keep track of which is F1, F2\n",
    "    F1_long = None\n",
    "    # determine which is longer\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        longer, shorter = F1, F2\n",
    "        F1_long = True\n",
    "    else:\n",
    "        longer, shorter = F2, F1\n",
    "        F1_long = False\n",
    "\n",
    "    downsampled = np.zeros(shorter.shape)\n",
    "    # determine factor and corresponding indices\n",
    "    factor_down = longer.shape[1]/shorter.shape[1]\n",
    "    idx_down = [round(factor_down*i) for i in range(shorter.shape[1])]\n",
    "\n",
    "    for i in range(shorter.shape[1]): # for each column in new empty array, get corresponding idx to use to select value in longer\n",
    "        idx = idx_down[i]\n",
    "        downsampled[:,i] = longer[:,idx] # copy values over\n",
    "    \n",
    "    # get original variables\n",
    "    if F1_long:\n",
    "        F1 = downsampled\n",
    "    else:\n",
    "        F2 = downsampled\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "\n",
    "    # convert back to same dimensions\n",
    "    if F1_long:\n",
    "        wp[0] = wp[0]*factor_down\n",
    "    else:\n",
    "        wp[1] = wp[1]*factor_down\n",
    "    #\n",
    "    times.append(time.time())\n",
    "\n",
    "    # write to pkl file\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW1_downsampleInterpolate(featfile1, featfile2, steps, weights, downsample, outfile = None, profile = False):\n",
    "    '''Same as DTW_downsample_quantized with one difference: rather than assigning the nearest feature vector, \n",
    "    handle fractional positions by doing a linear interpolation between the two bordering feature vectors'''\n",
    "    \n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "\n",
    "    shorter, longer = None, None\n",
    "    \n",
    "    # boolean to keep track of which is F1, F2\n",
    "    F1_long = None\n",
    "    # determine which is longer\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        longer, shorter = F1, F2\n",
    "        F1_long = True\n",
    "    else:\n",
    "        longer, shorter = F2, F1\n",
    "        F1_long = False\n",
    "\n",
    "    downsampled = np.zeros(shorter.shape)\n",
    "    # determine factor and corresponding indices\n",
    "    factor_down = longer.shape[1]/shorter.shape[1]\n",
    "    idx_down = [factor_down*i for i in range(shorter.shape[1])]\n",
    "\n",
    "    for i in range(shorter.shape[1]): # for each column in new empty array, get corresponding idx to use to select value in longer\n",
    "        idx = idx_down[i]\n",
    "        fraction1 = idx - math.floor(idx) # to use with higher number. e.g. if 1.73, then fraction1 = 0.73\n",
    "        fraction2 = 1 - fraction1\n",
    "        downsampled[:,i] = longer[:,math.floor(idx)]*fraction2 + longer[:,math.ceil(idx)]*fraction1 # copy values over with linear interpolation\n",
    "\n",
    "    # get original variables\n",
    "    if F1_long:\n",
    "        F1 = downsampled\n",
    "    else:\n",
    "        F2 = downsampled\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "\n",
    "    # convert back to same dimensions\n",
    "    if F1_long:\n",
    "        wp[0] = wp[0]*factor_down\n",
    "    else:\n",
    "        wp[1] = wp[1]*factor_down\n",
    "    #\n",
    "    times.append(time.time())\n",
    "\n",
    "    # write to pkl file\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW1_upsampleQuantized(featfile1, featfile2, steps, weights, downsample, outfile = None, profile = False):\n",
    "    '''For each position in the upsampled sequence, assign the nearest feature vector (to handle fractional positions)'''\n",
    "\n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "    \n",
    "    shorter, longer = None, None\n",
    "    \n",
    "    # boolean to keep track of which is F1, F2\n",
    "    F1_long = None\n",
    "    # determine which is longer\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        longer, shorter = F1, F2\n",
    "        F1_long = True\n",
    "    else:\n",
    "        longer, shorter = F2, F1\n",
    "        F1_long = False\n",
    "\n",
    "    upsampled = np.zeros(longer.shape)\n",
    "\n",
    "    # determine factor and corresponding indices\n",
    "    factor_up = shorter.shape[1]/longer.shape[1]\n",
    "    max_idx = shorter.shape[1] - 1\n",
    "    idx_up = [] # want to get indices\n",
    "    for i in range(longer.shape[1]): # for i in range longer\n",
    "        idx = round(factor_up*i)\n",
    "        if idx > max_idx: # need to check that index not out of range\n",
    "            idx -= 1\n",
    "        idx_up.append(idx)\n",
    "\n",
    "    for i in range(longer.shape[1]): # for each column in new empty array, get corresponding idx to use to select value in shorter\n",
    "        idx = idx_up[i]\n",
    "        upsampled[:,i] = shorter[:,idx] # copy values over\n",
    "    \n",
    "    # get original variables\n",
    "    if F1_long:\n",
    "        F2 = upsampled\n",
    "    else:\n",
    "        F1 = upsampled\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "\n",
    "    # convert back to same dimensions\n",
    "    if F1_long:\n",
    "        wp[1] = wp[1]*factor_up\n",
    "    else:\n",
    "        wp[0] = wp[0]*factor_up\n",
    "    #\n",
    "    times.append(time.time())\n",
    "\n",
    "    # write to pkl file\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTW1_upsampleInterpolate(featfile1, featfile2, steps, weights, downsample, outfile = None, profile = False):\n",
    "    '''Same as DTW_upsample_quantized with one difference: rather than assigning the nearest feature vector, \n",
    "    handle fractional positions by doing a linear interpolation between the two bordering feature vectors'''\n",
    "    \n",
    "    F1 = np.load(featfile1) # 12 x N\n",
    "    F2 = np.load(featfile2) # 12 x M\n",
    "    \n",
    "    shorter, longer = None, None\n",
    "    \n",
    "    # boolean to keep track of which is F1, F2\n",
    "    F1_long = None\n",
    "    # determine which is longer\n",
    "    if F1.shape[1] >= F2.shape[1]:\n",
    "        longer, shorter = F1, F2\n",
    "        F1_long = True\n",
    "    else:\n",
    "        longer, shorter = F2, F1\n",
    "        F1_long = False\n",
    "\n",
    "    upsampled = np.zeros(longer.shape)\n",
    "\n",
    "    # determine factor and corresponding indices\n",
    "    factor_up = shorter.shape[1]/longer.shape[1]\n",
    "    max_idx = shorter.shape[1] - 1\n",
    "    idx_up = [factor_up*i for i in range(longer.shape[1])]\n",
    "\n",
    "    for i in range(longer.shape[1]): # for each column in new empty array, get corresponding idx to use to select value in longer\n",
    "        idx = idx_up[i]\n",
    "        if math.ceil(idx) > max_idx: # check for index out of range\n",
    "            upsampled[:,i] = shorter[:,math.floor(idx)]\n",
    "        else:\n",
    "            fraction1 = idx - math.floor(idx) # to use with higher number. e.g. if 1.73, then fraction1 = 0.73\n",
    "            fraction2 = 1 - fraction1\n",
    "            upsampled[:,i] = shorter[:,math.floor(idx)]*fraction2 + shorter[:,math.ceil(idx)]*fraction1 # copy values over with linear interpolation\n",
    "    \n",
    "    # get original variables\n",
    "    if F1_long:\n",
    "        F2 = upsampled\n",
    "    else:\n",
    "        F1 = upsampled\n",
    "\n",
    "    # just same code as alignDTW\n",
    "    times = []\n",
    "    times.append(time.time())\n",
    "    C = 1 - F1[:,0::downsample].T @ F2[:,0::downsample] # cos distance metric\n",
    "    times.append(time.time())\n",
    "\n",
    "    dn = steps[:,0].astype(np.uint32)\n",
    "    dm = steps[:,1].astype(np.uint32)\n",
    "    parameters = {'dn': dn, 'dm': dm, 'dw': weights, 'SubSequence': False}\n",
    "    [D, s] = DTW_Cost_To_AccumCostAndSteps(C, parameters)\n",
    "    times.append(time.time())\n",
    "    [wp, endCol, endCost] = DTW_GetPath(D, s, parameters)\n",
    "\n",
    "    # convert back to same dimensions\n",
    "    if F1_long:\n",
    "        wp[1] = wp[1]*factor_up\n",
    "    else:\n",
    "        wp[0] = wp[0]*factor_up\n",
    "    #\n",
    "    times.append(time.time())\n",
    "\n",
    "    # write to pkl file\n",
    "    if outfile:\n",
    "        pickle.dump(wp, open(outfile, 'wb'))\n",
    "    \n",
    "    if profile:\n",
    "        return wp, np.diff(times)\n",
    "    else:\n",
    "        return wp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set DTW algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_DTW_settings(alignment):\n",
    "    '''given an alignment type, return steps, weights, alignment_function (function to use given the alignment type specified)'''\n",
    "    n_cores = 1\n",
    "    steps = None\n",
    "    weights = None\n",
    "    downsample = 1\n",
    "    alignment_function = None\n",
    "\n",
    "    if alignment == 'DTW1':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3])\n",
    "        alignment_function = alignDTW\n",
    "    elif alignment == 'DTW2':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([1,2,2])\n",
    "        alignment_function = alignDTW\n",
    "    elif alignment == 'DTW3':\n",
    "        steps = np.array([0,1,1,0,1,1]).reshape((-1,2))\n",
    "        weights = np.array([1,1,2])\n",
    "        alignment_function = alignDTW\n",
    "    elif alignment == 'DTW4':\n",
    "        steps = np.array([0,1,1,0,1,1]).reshape((-1,2))\n",
    "        weights = np.array([1,1,1])\n",
    "        alignment_function = alignDTW\n",
    "    elif alignment == 'DTW5':\n",
    "        steps = np.array([0,1,1,0]).reshape((-1,2))\n",
    "        weights = np.array([1,1])\n",
    "        alignment_function = alignDTW\n",
    "\n",
    "    elif alignment == 'DTW1_add3':\n",
    "        steps = np.array([1,1,1,2,2,1,1,3,3,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3,4,4])\n",
    "        alignment_function = alignDTW\n",
    "    elif alignment == 'DTW1_add4':\n",
    "        steps = np.array([1,1,1,2,2,1,1,3,3,1,1,4,4,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3,4,4,5,5])\n",
    "        alignment_function = alignDTW\n",
    "\n",
    "    elif alignment == 'DTW1_downsampleQuantized':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3])\n",
    "        alignment_function = DTW1_downsampleQuantized\n",
    "    elif alignment == 'DTW1_downsampleInterpolate':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3])\n",
    "        alignment_function = DTW1_downsampleInterpolate\n",
    "    elif alignment == 'DTW1_upsampleQuantized':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3])\n",
    "        alignment_function = DTW1_upsampleQuantized\n",
    "    elif alignment == 'DTW1_upsampleInterpolate':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        weights = np.array([2,3,3])\n",
    "        alignment_function = DTW1_upsampleInterpolate\n",
    "    elif alignment == 'DTW_adaptiveWeight1':\n",
    "        steps = np.array([1,1,1,2,2,1]).reshape((-1,2))\n",
    "        alignment_function = DTW_adaptiveWeight1\n",
    "    elif alignment == 'DTW_adaptiveWeight2':\n",
    "        steps = np.array([0,1,1,0,1,1]).reshape((-1,2))\n",
    "        alignment_function = DTW_adaptiveWeight2\n",
    "    elif alignment == 'DTW_selectiveTransitions':\n",
    "        steps = np.array([0,1,1,0,1,1]).reshape((-1,2))\n",
    "        weights = np.array([1,1,2])\n",
    "        alignment_function = alignDTW_selective\n",
    "\n",
    "    return n_cores, steps, weights, downsample, alignment_function\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Benchmarks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_benchmark(filelist, folder1, folder2, alignment, warpMax=None, outdir='experiments_test'):\n",
    "    '''runs the specified DTW algorithm for a single folder'''\n",
    "    # prepare inputs for the align batch function\n",
    "    query_list = 'cfg_files/filelist.{}.txt'.format(filelist) # make 'train_toy' to actual file path --> 'cfg_files/filelist.train_toy.txt'\n",
    "    featdir1 = Path('{}/features/clean'.format(folder1))\n",
    "    featdir2 = Path('{}/features/clean'.format(folder2))\n",
    "    median1 = ''.join(folder1[-6:].split('.')) # make 'Mazurkas_median_x1.000' --> x1000 to simplify out folder name\n",
    "    median2 = ''.join(folder2[-6:].split('.'))\n",
    "    if warpMax:\n",
    "        outdir = Path('{}/{}.{}.{}.{}{}'.format(outdir, filelist, median1, median2, alignment, warpMax))\n",
    "    else:\n",
    "        outdir = Path('{}/{}.{}.{}.{}'.format(outdir, filelist, median1, median2, alignment))\n",
    "    \n",
    "    # get alignment settings based on specified alignment type\n",
    "    n_cores, steps, weights, downsample, alignment_function = get_DTW_settings(alignment)\n",
    "\n",
    "    if warpMax: # selective transitions\n",
    "        alignDTW_batch(query_list, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, alignment_function, warpMax) # alignDTW_batch works to align all files listed in the filelist\n",
    "    else:\n",
    "        alignDTW_batch(query_list, featdir1, featdir2, outdir, n_cores, steps, weights, downsample, alignment_function)\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_benchmark(settings, warpMax=None): # takes in a list of lists. each list inside is a setting\n",
    "    '''runs multiple benchmarks'''\n",
    "    for setting in settings:\n",
    "        filelist, folder1, folder2, alignment = setting[0], setting[1], setting[2], setting[3]\n",
    "        single_benchmark(filelist, folder1, folder2, alignment, warpMax)\n",
    "        if warpMax:\n",
    "            print(\"Finished alignment for: {} {} {} {} {}\".format(filelist, folder1, folder2, alignment, warpMax))\n",
    "        else:\n",
    "            print(\"Finished alignment for: {} {} {} {}\".format(filelist, folder1, folder2, alignment))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments(filelist, alignments_list):\n",
    "    '''create a list of lists to pass into multiple_benchmark based on what alignments we want'''\n",
    "    all_arguments = []\n",
    "    for alignment in alignments_list:\n",
    "        to_concat = [[filelist, 'Mazurkas_median_x1.000', 'Mazurkas_median_x1.000', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x1.260', 'Mazurkas_median_x0.794', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x1.588', 'Mazurkas_median_x0.794', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x1.588', 'Mazurkas_median_x0.630', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x2.000', 'Mazurkas_median_x0.630', alignment],\n",
    "                    [filelist, 'Mazurkas_median_x2.000', 'Mazurkas_median_x0.500', alignment]]\n",
    "        all_arguments = all_arguments + to_concat\n",
    "    return all_arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple_benchmark([['train_toy', 'Mazurkas_median_x1.000', 'Mazurkas_median_x1.000', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x0.794', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x1.588', 'Mazurkas_median_x0.794', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x1.588', 'Mazurkas_median_x0.630', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x2.000', 'Mazurkas_median_x0.630', 'DTW1'],\n",
    "#                     ['train_toy', 'Mazurkas_median_x2.000', 'Mazurkas_median_x0.500', 'DTW1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_normal = get_arguments('train_toy', ['DTW1','DTW2','DTW3'])\n",
    "multiple_benchmark(DTW_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_adds = get_arguments('train_toy', ['DTW1_add3', 'DTW1_add4'])\n",
    "multiple_benchmark(DTW_adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished alignment for: train_toy Mazurkas_median_x1.260 Mazurkas_median_x1.000 DTW1_add4\n"
     ]
    }
   ],
   "source": [
    "# # TESTING\n",
    "# multiple_benchmark([['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', 'DTW1_add4']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_samples = get_arguments('train_toy', ['DTW1_downsampleQuantized', 'DTW1_downsampleInterpolate', 'DTW1_upsampleQuantized', 'DTW1_upsampleInterpolate'])\n",
    "multiple_benchmark(DTW_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished alignment for: train_toy Mazurkas_median_x1.260 Mazurkas_median_x1.000 DTW1_upsampleQuantized\n"
     ]
    }
   ],
   "source": [
    "# # TESTING\n",
    "# multiple_benchmark([['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', 'DTW1_upsampleQuantized']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_weights = get_arguments('train_toy', ['DTW_adaptiveWeight1', 'DTW_adaptiveWeight2'])\n",
    "multiple_benchmark(DTW_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTING\n",
    "# multiple_benchmark([['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', 'DTW_adaptiveWeight1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTW_selective = get_arguments('train_toy', ['DTW_selectiveTransitions']) # Only 'DTW_selectiveTransitions' since all variations use the same algorithm\n",
    "multiple_benchmark(DTW_selective, 2) # Note: works differently than the other variations, since has an extra changing parameter warpMax, but with the same algorithm\n",
    "multiple_benchmark(DTW_selective, 3)\n",
    "multiple_benchmark(DTW_selective, 4)\n",
    "multiple_benchmark(DTW_selective, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished alignment for: train_toy Mazurkas_median_x1.260 Mazurkas_median_x1.000 DTW_selectiveTransitions 5\n"
     ]
    }
   ],
   "source": [
    "# # TESTING\n",
    "# multiple_benchmark([['train_toy', 'Mazurkas_median_x1.260', 'Mazurkas_median_x1.000', 'DTW_selectiveTransitions']], 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_benchmark('test_full', '', '', 'DTW1') # two empty arguments since 'Chopin_Mazurka' used. And, need to delete '/' before features in 'featdir1 = Path('{}/features/clean'.format(folder1))'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
