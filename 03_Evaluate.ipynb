{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Evaluation Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import librosa as lb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate hypothesis directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(annotfile, subseq=False, offset=None, length=None):\n",
    "    df = pd.read_csv(annotfile, header=None, sep='\\s+', skiprows=3)\n",
    "    timestamps = np.array(df[0])\n",
    "    if subseq:\n",
    "        return timestamps, int(df[2][0].split(\"-\")[-1]), len(timestamps)\n",
    "    if offset:\n",
    "        return timestamps[offset:offset+length], None, None\n",
    "    return timestamps, None, None\n",
    "\n",
    "def eval_file(hypfile, hop_sec, annotation_root1, annotation_root2):\n",
    "\n",
    "    piece1, piece2 = os.path.basename(hypfile).split('__') # ['Chopin_Op017No4_Fou-1978_pid9071-20', 'Chopin_Op017No4_Magaloff-1977_pid5667267b-10.pkl']\n",
    "    mazurka = '_'.join(os.path.basename(piece1).split('_')[0:2]) # Chopin_Op017No4\n",
    "    annotation1 = (Path(annotation_root1) / mazurka / piece1).with_suffix('.beat') # Mazurkas_median_x1.260_subseq20/annotations_beat/Chopin_Op017No4/Chopin_Op017No4_Fou-1978_pid9071-20.beat\n",
    "    annotation2 = (Path(annotation_root2) / mazurka / piece2).with_suffix('.beat') # Mazurkas_median_x0.794/annotations_beat/Chopin_Op017No4/Chopin_Op017No4_Magaloff-1977_pid5667267b-10.beat\n",
    "\n",
    "    # check if first annotation file is a subsequence file\n",
    "    subseq = True if \"subseq20\" in str(annotation1) else False\n",
    "    t1, offset, length = get_timestamps(annotation1, subseq, None, None)\n",
    "    t2, _, _ = get_timestamps(annotation2, False, offset, length)\n",
    "\n",
    "    hypalign = pd.read_pickle(hypfile) # warping path in frames\n",
    "    return [] if hypalign is None else np.interp(t1, hypalign[0,:]*hop_sec, hypalign[1,:]*hop_sec) - t2\n",
    "\n",
    "def eval_dir(hypdir, querylist, hop_sec, annotation_root1, annotation_root2, savefile = None):\n",
    "    all_errs, count = {}, 0\n",
    "    print(f'Processing {hypdir} ', end='')\n",
    "    with open(querylist, 'r') as f:\n",
    "        for line in f:\n",
    "            piece1, piece2 = line.strip().split()\n",
    "            basename = os.path.basename(piece1) + '__' + os.path.basename(piece2)\n",
    "            hypfile = hypdir + '/' + basename + '.pkl'\n",
    "            if not os.path.exists(hypfile):\n",
    "                print(\"X\", end='')\n",
    "                continue\n",
    "            all_errs[basename] = eval_file(hypfile, hop_sec, annotation_root1, annotation_root2)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                print(\".\", end='')\n",
    "    print(' done')\n",
    "    if savefile:\n",
    "        pickle.dump(all_errs, open(savefile, 'wb'))\n",
    "\n",
    "def eval_all_dirs(rootdir, querylist, hop_sec, outdir):\n",
    "    if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    dataset = str(querylist).split('.')[1] # train_toy\n",
    "    for hypdir in glob.glob(f'{rootdir}/{dataset}*'):\n",
    "        savefile = outdir + '/' + os.path.basename(hypdir) + '.pkl'\n",
    "        _, piece1, piece2, subseq = hypdir.split('.')\n",
    "        subseq = \"_subseq20\" if \"Sub\" in subseq else \"\"\n",
    "        annotation_root1 = f'Mazurkas_median_{piece1[:2]}.{piece1[2:]}{subseq}/annotations_beat'\n",
    "        annotation_root2 = f'Mazurkas_median_{piece2[:2]}.{piece2[2:]}/annotations_beat'\n",
    "        eval_dir(hypdir, querylist, hop_sec, annotation_root1, annotation_root2, savefile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATIONS_ROOT = Path('Chopin_Mazurkas/annotations_beat')\n",
    "query_list = Path('cfg_files/query.test.list')\n",
    "query_list_train_toy = Path('cfg_files/filelist.train_toy.txt')\n",
    "query_list_train_small = Path('cfg_files/filelist.train_small.txt')\n",
    "query_list_train_medium = Path('cfg_files/filelist.train_medium.txt')\n",
    "query_list_train_full = Path('cfg_files/filelist.train_full.txt')\n",
    "query_list_test_full = Path('cfg_files/filelist.test_full.txt')\n",
    "\n",
    "EXPERIMENTS_ROOT = 'experiments_test'\n",
    "hop_sec = 512 * 1 / 22050\n",
    "outdir = 'evaluations_test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all directories with train_toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing experiments_test/train_toy.x1588.x0630.DTW1  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW3  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_selectiveTransitions5  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW2  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW3  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW3  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x0794.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_adaptiveWeight1  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW3  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW3  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW2  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW3  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW3  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_upsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW5  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_add4  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW2  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW1  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_adaptiveWeight1 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_selectiveTransitions4  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_adaptiveWeight2 XXXXXXXXXXXXXXX done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_add3  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1000.x1000.SubDTW4  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_downsampleQuantized  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.DTW_selectiveTransitions2  done\n",
      "Processing experiments_test/train_toy.x2000.x0500.DTW2  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW6  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.SubDTW3  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_subseq  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1  done\n",
      "Processing experiments_test/train_toy.x1260.x1000.DTW1_downsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW2  done\n",
      "Processing experiments_test/train_toy.x1260.x0794.DTW_selectiveTransitions3  done\n",
      "Processing experiments_test/train_toy.x1588.x0630.SubDTW7  done\n",
      "Processing experiments_test/train_toy.x2000.x0630.DTW1_upsampleInterpolate  done\n",
      "Processing experiments_test/train_toy.x1588.x0794.SubDTW1  done\n"
     ]
    }
   ],
   "source": [
    "eval_all_dirs(EXPERIMENTS_ROOT, query_list_train_toy, hop_sec, outdir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot error vs tolerance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper functions for plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_time_warp(num1, num2):\n",
    "    ''' calculate global time warp from two numbers'''\n",
    "    return round(num1/num2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_basename(dir):\n",
    "    '''get system, global_time_warp from basename (name of a folder)'''\n",
    "    info = dir.split('.')\n",
    "    system = info[3]\n",
    "    num1 = float('{}.{}'.format(info[1][1], info[1][2:]))\n",
    "    num2 = float('{}.{}'.format(info[2][1], info[2][2:]))\n",
    "    global_time_warp = get_global_time_warp(num1, num2)\n",
    "    return system, global_time_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_names(filelist, alignments_list):\n",
    "    '''create a list of lists of folder names corresponding to filelist and alignment types'''\n",
    "    all_folders = []\n",
    "    for alignment in alignments_list:\n",
    "        to_concat = ['{}.x1000.x1000.{}'.format(filelist, alignment), '{}.x1260.x0794.{}'.format(filelist, alignment), '{}.x1260.x1000.{}'.format(filelist, alignment),\n",
    "                     '{}.x1588.x0630.{}'.format(filelist, alignment), '{}.x1588.x0794.{}'.format(filelist, alignment),\n",
    "                     '{}.x2000.x0630.{}'.format(filelist, alignment), '{}.x2000.x0500.{}'.format(filelist, alignment)]\n",
    "        \n",
    "        all_folders = all_folders + to_concat\n",
    "    return all_folders\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates(errFile, maxTol):\n",
    "    # read from file\n",
    "    with open(errFile, 'rb') as f:\n",
    "        allErrs = pickle.load(f)\n",
    "    \n",
    "    # collect all errors\n",
    "    errsFlat = []\n",
    "    for query in allErrs:\n",
    "        errs = np.array(allErrs[query])\n",
    "        errsFlat.append(errs)\n",
    "    errsFlat = np.concatenate(errsFlat)\n",
    "    \n",
    "    # calculate error rates\n",
    "    errRates = np.zeros(maxTol+1)\n",
    "    for i in range(maxTol+1):\n",
    "        errRates[i] = np.mean(np.abs(errsFlat) > i/1000)\n",
    "    \n",
    "    return errRates, errsFlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error_rates_batch(indir, basenames, maxTol):\n",
    "    errRates = np.zeros((len(basenames), maxTol+1))\n",
    "    allErrVals = []\n",
    "    print('Computing error rates ', end='')\n",
    "    for i, basename in enumerate(basenames):\n",
    "        errFile = indir + '/' + basename + '.pkl'\n",
    "        errRates[i,:], errors = calc_error_rates(errFile, maxTol)\n",
    "        allErrVals.append(errors)\n",
    "        print('.', end='')\n",
    "    print(' done')\n",
    "    return errRates, allErrVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_roc(errRates, basenames):\n",
    "    numSystems = errRates.shape[0]\n",
    "    maxTol = errRates.shape[1] - 1\n",
    "    for i in range(numSystems):\n",
    "        plt.plot(np.arange(maxTol+1), errRates[i,:] * 100.0)\n",
    "        \n",
    "    # create appropriate legend names corresponding to basenames\n",
    "    legend = []\n",
    "    for folder in basenames:\n",
    "        system, global_time_warp = get_info_from_basename(folder)\n",
    "        legend.append('Global Time Warp = {}, {}'.format(global_time_warp, system))\n",
    "    \n",
    "    plt.legend(legend, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    \n",
    "    plt.xlabel('Error Tolerance (ms)')\n",
    "    plt.ylabel('Error Rate (%)')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: change the 'toPlot' list to include only the alignment algorithms we've run so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_ROOT_DIR = 'evaluations_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot = get_folder_names('train_toy', ['DTW1', 'DTW2', 'DTW3', 'DTW1_add3', 'DTW1_add4', 'DTW1_downsampleQuantized','DTW1_downsampleInterpolate','DTW1_upsampleQuantized','DTW1_upsampleInterpolate', 'DTW_adaptiveWeight1', 'DTW_adaptiveWeight2', 'DTW_selectiveTransitions3'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates, errVals = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot, maxTol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_multiple_roc(errRates, toPlot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Grouped Barplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphing all systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(errRates, basenames, tols):\n",
    "    '''generates df to use with histogram'''\n",
    "    data = []\n",
    "    for i, dir in enumerate(basenames):\n",
    "        # get system from basename\n",
    "        system, global_time_warp = get_info_from_basename(dir)\n",
    "        # for tol in tols:\n",
    "        for tol in tols:\n",
    "            # get errors and append\n",
    "            data.append((system, tol, errRates[i,tol]*100, global_time_warp))\n",
    "    df = pd.DataFrame(data, columns = ['System', 'Tolerance', 'Error', 'Global Time Warp'])\n",
    "\n",
    "    # check for NaN which indicates a false path and set error to 100\n",
    "    #df.fillna(100, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df_with_tols(errRates, basenames, tols):\n",
    "    '''generates df to use with histogram'''\n",
    "    data = []\n",
    "    for i, dir in enumerate(basenames):\n",
    "        # get system from basename\n",
    "        system, global_time_warp = get_info_from_basename(dir)\n",
    "\n",
    "\n",
    "        # get errors and append\n",
    "        data.append((system, errRates[i,100]*100, errRates[i,200]*100, errRates[i,500]*100, global_time_warp))\n",
    "    df = pd.DataFrame(data, columns = ['System', 'Tolerance 100 Error', 'Tolerance 200 Error','Tolerance 500 Error', 'Global Time Warp'])\n",
    "\n",
    "    # check for NaN which indicates a false path and set error to 100\n",
    "    #df.fillna(100, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grouped_barplot(df):  \n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    tol_200 = df[df['Tolerance']==200]\n",
    "    graph200 = sns.barplot(data=tol_200, x=\"Global Time Warp\", y=\"Error\", hue=\"System\", palette=\"Paired\")\n",
    "    \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xlabel(\"Global Time Warp\", size=16)\n",
    "    plt.ylabel(\"Error Rate\", size=16)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_tols(df):\n",
    "    fig, ax = plt.subplots(figsize=(20, 12))\n",
    "    sns.barplot(data=df, x=\"Global Time Warp\", y=\"Tolerance 200 Error\", hue=\"System\", palette=\"Paired\", ax=ax)\n",
    "\n",
    "    for ix, a in enumerate(ax.patches):\n",
    "        x_start = a.get_x()\n",
    "        width = a.get_width()\n",
    "        ax.plot([x_start, x_start+width], 2*[df.loc[ix, 'Tolerance 500 Error']], '-', c='k')\n",
    "        ax.plot([x_start, x_start+width], 2*[df.loc[ix, 'Tolerance 100 Error']], '-', c='k')\n",
    "\n",
    "    plt.xlabel(\"Global Time Warp\", size=16)\n",
    "    plt.ylabel(\"Error Rate\", size=16)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = generate_df(errRates, toPlot, [200, 500, 100])\n",
    "plot_grouped_barplot(df1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Error Tolerances 100 and 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_tols = generate_df_with_tols(errRates, toPlot, [200, 500, 100])\n",
    "plot_with_tols(df1_tols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph for DTW1, DTW2, DTW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot_normal = get_folder_names('train_toy',['DTW1', 'DTW2', 'DTW3'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates_normal, errVals_normal = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot_normal, maxTol)\n",
    "df_normal = generate_df_with_tols(errRates_normal, toPlot_normal, [200, 500, 100])\n",
    "plot_with_tols(df_normal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph for DTW1_add3, DTW1_add4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot_add = get_folder_names('train_toy',['DTW1', 'DTW1_add3', 'DTW1_add4'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates_add, errVals_add = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot_add, maxTol)\n",
    "df_add = generate_df_with_tols(errRates_add, toPlot_add, [200, 500, 100])\n",
    "plot_with_tols(df_add)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph for downsample and upsample DTWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot_sample = get_folder_names('train_toy',['DTW_selectiveTransitions5', 'DTW1_downsampleQuantized', 'DTW1_downsampleInterpolate','DTW1_upsampleQuantized','DTW1_upsampleInterpolate'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates_sample, errVals_sample = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot_sample, maxTol)\n",
    "df_sample = generate_df_with_tols(errRates_sample, toPlot_sample, [200, 500, 100])\n",
    "plot_with_tols(df_sample)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph for adaptive weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot_weight = get_folder_names('train_toy',['DTW1', 'DTW_adaptiveWeight1', 'DTW_adaptiveWeight2'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates_weight, errVals_weight = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot_weight, maxTol)\n",
    "df_weight = generate_df_with_tols(errRates_weight, toPlot_weight, [200, 500, 100])\n",
    "plot_with_tols(df_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph for selective transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPlot_selective = get_folder_names('train_toy',['DTW1', 'DTW_selectiveTransitions2', 'DTW_selectiveTransitions3', 'DTW_selectiveTransitions4', 'DTW_selectiveTransitions5'])\n",
    "maxTol = 1000 # in msec\n",
    "errRates_selective, errVals_selective = calc_error_rates_batch(EVAL_ROOT_DIR, toPlot_selective, maxTol)\n",
    "df_selective = generate_df_with_tols(errRates_selective, toPlot_selective, [200, 500, 100])\n",
    "plot_with_tols(df_selective)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
